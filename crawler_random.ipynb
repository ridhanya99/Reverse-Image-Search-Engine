{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bite8e95c26acc149f0a6ff9fe9bde07444",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Crawling Random images.."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from math import ceil\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from cffi.api import basestring\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "source": [
    "## ImagesDownloader: get a list of links, download the images and order them in a folder.."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder_existance(folderpath, throw_error_if_no_folder=False, display_msg=True):\n",
    "    \"\"\" check if a folder exists.\n",
    "        If throw_error_if_no_folder = True and the folder does not exist:\n",
    "            the method will print an error message and stop the program,\n",
    "        Otherwise:\n",
    "            the method will create the folder\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folderpath):\n",
    "        if throw_error_if_no_folder:\n",
    "            print(\"Error: folder '\" + folderpath + \"' does not exist\")\n",
    "            exit()\n",
    "        else:\n",
    "            os.makedirs(folderpath)\n",
    "            if display_msg:\n",
    "                print(\"Target folder '\", folderpath, \"' does not exist...\")\n",
    "                print(\" >> Folder created\")\n",
    "\n",
    "\n",
    "class ImagesDownloader(object):\n",
    "    \"\"\"Download a list of images, rename them and save them to the specified folder\"\"\"\n",
    "\n",
    "    images_links = []\n",
    "    failed_links = []\n",
    "    default_target_folder = 'images'\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"Preparing to download images...\")\n",
    "\n",
    "    def download(self, links, target_folder='./data'):\n",
    "        \"\"\"Download images from a lisk of links\"\"\"\n",
    "\n",
    "        # check links and folder:\n",
    "        if len(links) < 1:\n",
    "            print(\"Error: Empty list, no links provided\")\n",
    "            exit()\n",
    "        self.images_links = links\n",
    "        check_folder_existance(target_folder)\n",
    "        if target_folder[-1] == '/':\n",
    "            target_folder = target_folder[:-1]\n",
    "\n",
    "        # start downloading:\n",
    "        print(\"Downloading files...\")\n",
    "        progress = 0\n",
    "        images_nbr = len(self.images_links)\n",
    "        for link in self.images_links:\n",
    "            target_file = target_folder + '/' + link.split('/')[-1]\n",
    "            try:\n",
    "                f = urllib.request.URLopener()\n",
    "                f.retrieve(link, target_file)\n",
    "                    \n",
    "                #Resize the image and save\n",
    "                #image = imageio.imread(target_file)\n",
    "                #image_resized = np.array(Image.fromarray(image).resize( (128, 128)))\n",
    "                #imageio.imwrite(target_file, image_resized)\n",
    "\n",
    "            except IOError:\n",
    "                self.failed_links.append(link)\n",
    "            progress = progress + 1\n",
    "            print(\"\\r >> Download progress: \", (progress * 100 / images_nbr), \"%...\", end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        print(\"\\r >> Download progress: \", (progress * 100 / images_nbr), \"%\")\n",
    "        print(\" >> \", (progress - len(self.failed_links)), \" images downloaded\")\n",
    "\n",
    "        # save failed links:\n",
    "        if len(self.failed_links):\n",
    "            f2 = open(target_folder + \"/failed_list.txt\", 'w')\n",
    "            for link in self.failed_links:\n",
    "                f2.write(link + \"\\n\")\n",
    "            print(\" >> Failed to download \", len(self.failed_links),\n",
    "                  \" images: access not granted \",\n",
    "                  \"(links saved to: '\", target_folder, \"/failed_list.txt')\")"
   ]
  },
  {
   "source": [
    "## WebCrawler: fetch images from various search engines and download them.."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebCrawler(object):\n",
    "    \"\"\" Fetch images from various search engines and download them\"\"\"\n",
    "    api_keys = []\n",
    "    images_links = []\n",
    "    keywords = ''\n",
    "\n",
    "    def __init__(self, api_keys):\n",
    "        self.api_keys = api_keys\n",
    "\n",
    "    @staticmethod\n",
    "    def error(msg):\n",
    "        \"\"\"Display an error message and exit\"\"\"\n",
    "        print(\"Error: \", msg)\n",
    "        exit()\n",
    "\n",
    "    def collect_links_from_web(self, number_links_per_engine, remove_duplicated_links=False):\n",
    "        # validate params:\n",
    "        number_links = int(number_links_per_engine)\n",
    "        if number_links <= 0:\n",
    "            print(\"Warning: number_links_per_engine must be positive, value changed to default (100 links)\")\n",
    "            number_links = 100\n",
    "\n",
    "        # call methods for fetching image links in the selected search engines:\n",
    "        print(\"Start fetching...\")\n",
    "        extracted_links = []\n",
    "        for engine, keys in self.api_keys.items():\n",
    "            try:\n",
    "                method = getattr(self, 'fetch_from_' + engine)\n",
    "            except AttributeError:\n",
    "                self.error('funciton fetch_from_' + engine + '() not defined')\n",
    "            temporary_links = method(keys[0], keys[1], number_links)\n",
    "            extracted_links += temporary_links\n",
    "            print(\"\\r >> \", len(temporary_links), \" links extracted\", end=\"\\n\")\n",
    "\n",
    "        # remove duplicated links:\n",
    "        if remove_duplicated_links:\n",
    "            links_count = len(extracted_links)\n",
    "            extracted_links = list(set(extracted_links))\n",
    "            print(\" >> \", links_count - len(extracted_links),\n",
    "                \" duplicated links removed, \", len(extracted_links), \" kept\")\n",
    "\n",
    "        # store the links into the global list:\n",
    "        self.images_links = extracted_links\n",
    "\n",
    "\n",
    "    def fetch_from_flickr(self,api_key, api_secret, number_links=50):\n",
    "        \"\"\" Fetch random images from Flikr \"\"\"\n",
    "        from flickrapi import FlickrAPI \n",
    "        \n",
    "        random.seed()\n",
    "        rand_page = random.randint(1,50)   #4000 pages \n",
    "    \n",
    "        # calculate number of pages:\n",
    "        if number_links < 30:\n",
    "            items_per_page = number_links\n",
    "        else:\n",
    "            items_per_page = 30   # max 200 for flikr\n",
    "        pages_nbr = int(ceil(number_links / items_per_page))\n",
    "        links = []\n",
    "\n",
    "        # get links from the random page:\n",
    "        print(\"Carwling Flickr Search...\")\n",
    "        flickr = FlickrAPI(api_key, api_secret)\n",
    "        response = flickr.photos_search(api_key=api_key,\n",
    "                                        page=rand_page,\n",
    "                                        per_page=items_per_page,\n",
    "                                        media='photos',\n",
    "                                        sort='relevance')\n",
    "        images = [im for im in list(response.iter()) if im.tag == 'photo']\n",
    "        for photo in images:\n",
    "            photo_url = \"https://farm{0}.staticflickr.com/{1}/{2}_{3}.jpg\". format(\n",
    "                photo.get('farm'), photo.get('server'), photo.get('id'), photo.get('secret'))\n",
    "            links.append(photo_url)\n",
    "        print(\" >> \", len(links), \" links extracted...\", end=\"\")\n",
    "\n",
    "        # get next pages:\n",
    "        for i in range(1, pages_nbr):\n",
    "            response = flickr.photos_search(api_key=api_key,\n",
    "                                            page=rand_page+1,\n",
    "                                            per_page=items_per_page,\n",
    "                                            media='photos',  \n",
    "                                            sort='relevance')\n",
    "            images = [im for im in list(response.iter()) if im.tag == 'photo']\n",
    "            for photo in images:\n",
    "                link = \"https://farm{0}.staticflickr.com/{1}/{2}_{3}.jpg\". format(\n",
    "                    photo.get('farm'), photo.get('server'), photo.get('id'), photo.get('secret'))\n",
    "                links.append(link)\n",
    "            print(\"\\r >> \", len(links), \" links extracted...\", end=\"\")\n",
    "\n",
    "        # store and reduce the number of images if too much:\n",
    "        return links\n",
    "\n",
    "\n",
    "    def save_urls(self, filename):\n",
    "        \"\"\" Save links to disk \"\"\"\n",
    "        folder, _ = os.path.split(filename)\n",
    "        if filename and not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        with open(filename, 'w') as links_file:\n",
    "            for link in self.images_links:\n",
    "                links_file.write(link + '\\n')\n",
    "        print(\"\\nLinks saved to '\", filename, \"'\")\n",
    "\n",
    "    def load_urls(self, filename):\n",
    "        \"\"\" Load links from a file\"\"\"\n",
    "        if not os.path.isfile(filename):\n",
    "            self.error(\"Failed to load URLs, file '\" + filename + \"' does not exist\")\n",
    "        with open(filename) as links_file:\n",
    "            self.images_links= []\n",
    "            for link in links_file:\n",
    "                self.images_links.append(link)\n",
    "        print(\"\\nLinks loaded from \", filename)\n",
    "\n",
    "    def download_images(self, target_folder='./data'):\n",
    "        \"\"\" Download images and store them in the specified folder \"\"\"\n",
    "        print(\" \")\n",
    "        downloader = ImagesDownloader()\n",
    "        if not target_folder:\n",
    "            target_folder = './data'\n",
    "        downloader.download(self.images_links, target_folder)"
   ]
  },
  {
   "source": [
    "## Get Flickr API and crawl the web.."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_keys = {'flickr': ('24c5ab910f81210e9d70652814f367da', 'd52bd1a9550316d2')}\n",
    "images_nbr = 10                     # number of images to fetch\n",
    "download_folder = \"D:/XAMPP/htdocs/Package/Image_Retrieval/data/\" # folder in which the images will be stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start fetching...\n",
      "Carwling Flickr Search...\n",
      " >>  10  links extracted\n",
      " >>  0  duplicated links removed,  10  kept\n",
      "\n",
      "Links saved to ' ./data2/links.txt '\n",
      " \n",
      "Preparing to download images...\n",
      "Downloading files...\n",
      " >> Download progress:  100.0 %\n",
      " >>  10  images downloaded\n"
     ]
    }
   ],
   "source": [
    "crawler = WebCrawler(api_keys)\n",
    "\n",
    "# 1. Crawl the web and collect URLs:\n",
    "crawler.collect_links_from_web(images_nbr, remove_duplicated_links=True)\n",
    "\n",
    "# 2. Save URLs to download them later (optional):\n",
    "crawler.save_urls(download_folder + \"/links.txt\")\n",
    "\n",
    "#  (alernative to the previous line) Load URLs from a file instead of the web:\n",
    "#crawler.load_urls(download_folder + \"/links.txt\")\n",
    "\n",
    "# 3. Download the images:\n",
    "crawler.download_images(target_folder=download_folder)"
   ]
  }
 ]
}